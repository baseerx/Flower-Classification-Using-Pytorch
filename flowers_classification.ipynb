{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the current directory path\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "# Append the \"flowers\" folder to the directory path\n",
    "flowers_path = os.path.join(dir_path, \"flowers\")\n",
    "# Fetch the flowers from the relevant subfolder\n",
    "flowers = os.listdir(flowers_path)\n",
    "\n",
    "\n",
    "# Create a dictionary to store the flowers and their corresponding paths\n",
    "flower_paths = {}\n",
    "\n",
    "# Loop through the flowers\n",
    "for flower in flowers:\n",
    "    # Fetch the path of the flower\n",
    "    flower_path = os.path.join(flowers_path, flower)\n",
    "    # Fetch the images of the flower\n",
    "    flower_images = os.listdir(flower_path)\n",
    "    # Loop through the images\n",
    "    for image in flower_images:\n",
    "        # Fetch the path of the image\n",
    "        image_path = os.path.join(flower_path, image)\n",
    "        # Store the image path in the dictionary\n",
    "        flower_paths[image_path] = flower\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a dictionary with the image paths as keys and the flower names as values\n",
    "# We will convert into dataframe with keys as one column and values as another column\n",
    "flower_paths_df = pd.DataFrame.from_dict(flower_paths, orient='index')\n",
    "flower_paths_df = flower_paths_df.reset_index()\n",
    "flower_paths_df.columns = ['images', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\d...</td>\n",
       "      <td>daisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\d...</td>\n",
       "      <td>dandelion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\s...</td>\n",
       "      <td>sunflower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\d...</td>\n",
       "      <td>daisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\t...</td>\n",
       "      <td>tulip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images     labels\n",
       "0  d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\d...      daisy\n",
       "1  d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\d...  dandelion\n",
       "2  d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\s...  sunflower\n",
       "3  d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\d...      daisy\n",
       "4  d:\\BASEERX\\MS - Data Science\\Pytorch\\flowers\\t...      tulip"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "# Shuffle the dataframe\n",
    "flower_paths_df = flower_paths_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "flower_paths_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will split the dataframe into train, validation and test dataframes\n",
    "X_Train,X_Test,Y_Train,Y_Test = train_test_split(flower_paths_df['images'],flower_paths_df['labels'],test_size=0.2,random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#Convert the images from path to tensors\n",
    "def convert_image_to_tensor(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((224,224))\n",
    "    image = np.array(image)\n",
    "    image = torch.from_numpy(image)\n",
    "    image = image.permute(2,0,1)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "#Loop through the train images and convert them to tensors\n",
    "\n",
    "train_images = []\n",
    "for image_path in X_Train:\n",
    "    image = convert_image_to_tensor(image_path)\n",
    "    train_images.append(image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = torch.stack(train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[254, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255,  ..., 255, 255, 254],\n",
       "          [255, 255, 255,  ..., 255, 255, 254],\n",
       "          [255, 255, 255,  ..., 255, 255, 255]],\n",
       "\n",
       "         [[254, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255,  ..., 255, 255, 254],\n",
       "          [255, 255, 255,  ..., 255, 255, 254],\n",
       "          [255, 255, 255,  ..., 255, 255, 255]],\n",
       "\n",
       "         [[254, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255,  ..., 255, 255, 254],\n",
       "          [255, 255, 255,  ..., 255, 255, 254],\n",
       "          [255, 255, 255,  ..., 255, 255, 255]]],\n",
       "\n",
       "\n",
       "        [[[ 48,  53,  67,  ...,  13,  11,   9],\n",
       "          [ 50,  56,  68,  ...,  10,   9,   7],\n",
       "          [ 55,  62,  71,  ...,   6,   7,   5],\n",
       "          ...,\n",
       "          [ 62,  63,  70,  ...,  14,  14,  12],\n",
       "          [ 62,  64,  72,  ...,  14,  14,  12],\n",
       "          [ 63,  64,  72,  ...,  14,  13,  12]],\n",
       "\n",
       "         [[ 39,  42,  53,  ...,  32,  27,  23],\n",
       "          [ 41,  45,  55,  ...,  28,  24,  21],\n",
       "          [ 46,  51,  58,  ...,  24,  21,  20],\n",
       "          ...,\n",
       "          [ 47,  46,  50,  ...,  36,  37,  37],\n",
       "          [ 48,  48,  51,  ...,  38,  38,  37],\n",
       "          [ 50,  50,  53,  ...,  39,  38,  36]],\n",
       "\n",
       "         [[ 32,  36,  47,  ...,   5,   3,   0],\n",
       "          [ 34,  39,  49,  ...,   3,   2,   0],\n",
       "          [ 39,  45,  52,  ...,   1,   1,   0],\n",
       "          ...,\n",
       "          [ 42,  41,  49,  ...,  18,  18,  18],\n",
       "          [ 42,  43,  51,  ...,  20,  19,  19],\n",
       "          [ 42,  44,  52,  ...,  20,  19,  20]]],\n",
       "\n",
       "\n",
       "        [[[116,  72,  26,  ...,  91,  99,  83],\n",
       "          [133,  97,  69,  ...,  78,  85,  85],\n",
       "          [154, 148, 142,  ...,  68,  72,  78],\n",
       "          ...,\n",
       "          [ 51,  51,  33,  ...,  75,  77,  76],\n",
       "          [ 37,  51,  47,  ...,  74,  75,  84],\n",
       "          [ 12,  25,  47,  ...,  75,  74,  77]],\n",
       "\n",
       "         [[123, 101,  56,  ..., 118, 123, 102],\n",
       "          [144, 123,  99,  ..., 104, 114, 113],\n",
       "          [171, 169, 168,  ...,  92, 107, 119],\n",
       "          ...,\n",
       "          [ 71,  66,  44,  ..., 116, 110,  92],\n",
       "          [ 59,  72,  65,  ..., 114, 109, 111],\n",
       "          [ 36,  48,  69,  ..., 113, 108, 109]],\n",
       "\n",
       "         [[ 54,  25,   0,  ...,  43,  39,  31],\n",
       "          [ 64,  48,  22,  ...,  36,  37,  49],\n",
       "          [ 76,  89,  67,  ...,  34,  35,  57],\n",
       "          ...,\n",
       "          [ 55,  54,  29,  ...,  40,  45,  43],\n",
       "          [ 44,  56,  45,  ...,  34,  37,  50],\n",
       "          [ 20,  30,  50,  ...,  33,  36,  45]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[228, 246, 251,  ...,  46,  49, 120],\n",
       "          [227, 241, 255,  ...,  64,  83, 145],\n",
       "          [227, 236, 254,  ...,  76, 107, 149],\n",
       "          ...,\n",
       "          [135, 181, 190,  ..., 217, 208, 205],\n",
       "          [114, 162, 200,  ..., 214, 208, 203],\n",
       "          [128, 167, 216,  ..., 214, 206, 203]],\n",
       "\n",
       "         [[ 26,  60, 100,  ...,  86,  85, 144],\n",
       "          [ 26,  53,  97,  ...,  98, 113, 162],\n",
       "          [ 26,  41,  86,  ..., 107, 131, 161],\n",
       "          ...,\n",
       "          [107, 157, 164,  ...,  56,  56,  55],\n",
       "          [ 89, 141, 176,  ...,  57,  57,  57],\n",
       "          [103, 146, 193,  ...,  58,  58,  57]],\n",
       "\n",
       "         [[ 46,  69, 106,  ...,   0,  17,  86],\n",
       "          [ 50,  64, 105,  ...,  13,  49, 107],\n",
       "          [ 56,  58,  96,  ...,  29,  68, 105],\n",
       "          ...,\n",
       "          [ 77, 128, 138,  ...,  58,  58,  57],\n",
       "          [ 56, 111, 152,  ...,  60,  59,  59],\n",
       "          [ 70, 115, 170,  ...,  61,  60,  60]]],\n",
       "\n",
       "\n",
       "        [[[112, 117, 151,  ...,  31,  37,  42],\n",
       "          [113, 119, 151,  ...,  36,  42,  45],\n",
       "          [117, 123, 153,  ...,  46,  49,  50],\n",
       "          ...,\n",
       "          [108, 118, 133,  ...,  39,  44,  62],\n",
       "          [126, 127, 133,  ...,  38,  43,  60],\n",
       "          [135, 132, 134,  ...,  37,  42,  59]],\n",
       "\n",
       "         [[110, 115, 151,  ...,  40,  49,  52],\n",
       "          [111, 116, 151,  ...,  47,  54,  55],\n",
       "          [114, 120, 151,  ...,  60,  63,  60],\n",
       "          ...,\n",
       "          [108, 111, 122,  ...,  44,  54,  72],\n",
       "          [124, 120, 123,  ...,  43,  53,  71],\n",
       "          [133, 125, 124,  ...,  43,  52,  70]],\n",
       "\n",
       "         [[121, 137, 184,  ...,  30,  39,  45],\n",
       "          [122, 138, 183,  ...,  35,  42,  47],\n",
       "          [125, 142, 183,  ...,  43,  47,  51],\n",
       "          ...,\n",
       "          [118, 121, 130,  ...,  47,  56,  74],\n",
       "          [129, 124, 128,  ...,  44,  54,  73],\n",
       "          [135, 126, 128,  ...,  43,  53,  72]]],\n",
       "\n",
       "\n",
       "        [[[218, 216, 215,  ..., 220, 221, 219],\n",
       "          [218, 218, 218,  ..., 221, 219, 218],\n",
       "          [220, 219, 219,  ..., 221, 220, 217],\n",
       "          ...,\n",
       "          [194, 195, 198,  ..., 171, 165, 165],\n",
       "          [193, 197, 199,  ..., 168, 168, 168],\n",
       "          [197, 196, 197,  ..., 165, 168, 166]],\n",
       "\n",
       "         [[214, 214, 211,  ..., 203, 204, 204],\n",
       "          [216, 216, 214,  ..., 204, 204, 204],\n",
       "          [218, 217, 216,  ..., 204, 205, 203],\n",
       "          ...,\n",
       "          [143, 142, 142,  ..., 180, 178, 177],\n",
       "          [139, 141, 140,  ..., 179, 179, 177],\n",
       "          [139, 138, 136,  ..., 177, 181, 178]],\n",
       "\n",
       "         [[238, 238, 237,  ..., 218, 217, 217],\n",
       "          [239, 240, 239,  ..., 218, 218, 218],\n",
       "          [240, 241, 240,  ..., 218, 220, 219],\n",
       "          ...,\n",
       "          [189, 188, 189,  ..., 156, 154, 155],\n",
       "          [186, 188, 188,  ..., 156, 154, 154],\n",
       "          [187, 186, 186,  ..., 155, 155, 153]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Y_Train:\n",
    "    if i == 'daisy':\n",
    "        Y_Train.replace(i,0,inplace=True)\n",
    "    elif i == 'dandelion':\n",
    "        Y_Train.replace(i,1,inplace=True)\n",
    "    elif i == 'rose':\n",
    "        Y_Train.replace(i,2,inplace=True)\n",
    "    elif i == 'sunflower':\n",
    "        Y_Train.replace(i,3,inplace=True)\n",
    "    elif i == 'tulip':\n",
    "        Y_Train.replace(i,4,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unknown column Y_Train to tensor\n",
    "Y_Train = torch.from_numpy(np.array(Y_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now We have X_Train, Y_Train in tensor format we will create CNN model for flowers classification\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,3,1)\n",
    "        self.conv2 = nn.Conv2d(6,16,3,1)\n",
    "        self.fc1 = nn.Linear(54*54*16,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,5)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X,2,2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X,2,2)\n",
    "        X = X.view(-1,54*54*16)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1288, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4346, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6479, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3178, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0085, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2655, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0659, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0005, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1914, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0077, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the CNN model\n",
    "model = CNN()\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Define the training set\n",
    "train = torch.utils.data.TensorDataset(train_images,Y_Train)\n",
    "\n",
    "# Define the training data loader\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#tune the model for 100 epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in train_loader:\n",
    "        X,y = data\n",
    "        model.zero_grad()\n",
    "        output = model(X.float())\n",
    "        loss = criterion(output,y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the test images to tensors\n",
    "\n",
    "test_images = []\n",
    "for image_path in X_Test:\n",
    "    image = convert_image_to_tensor(image_path)\n",
    "    test_images.append(image)\n",
    "\n",
    "test_images = torch.stack(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the test labels to tensors\n",
    "\n",
    "for i in Y_Test:\n",
    "    if i == 'daisy':\n",
    "        Y_Test.replace(i,0,inplace=True)\n",
    "    elif i == 'dandelion':\n",
    "        Y_Test.replace(i,1,inplace=True)\n",
    "    elif i == 'rose':\n",
    "        Y_Test.replace(i,2,inplace=True)\n",
    "    elif i == 'sunflower':\n",
    "        Y_Test.replace(i,3,inplace=True)\n",
    "    elif i == 'tulip':\n",
    "        Y_Test.replace(i,4,inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Test = torch.from_numpy(np.array(Y_Test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5868055555555556\n"
     ]
    }
   ],
   "source": [
    "#Predicting the test images\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_images.float())\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "\n",
    "#Calculating the accuracy of the model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(Y_Test, y_pred)\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "torch.save(model.state_dict(), 'flowers_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[138], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#I have a test Image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_image_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_image.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[126], line 4\u001b[0m, in \u001b[0;36mconvert_image_to_tensor\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_image_to_tensor\u001b[39m(image_path):\n\u001b[1;32m----> 4\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m      6\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n",
      "File \u001b[1;32mc:\\Users\\baseerx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_image.jpg'"
     ]
    }
   ],
   "source": [
    "#I have a test Image\n",
    "test_image = convert_image_to_tensor('test_image.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[232, 232, 232,  ...,  54,  56,  55],\n",
       "          [230, 230, 230,  ...,  55,  54,  52],\n",
       "          [225, 225, 225,  ...,  56,  53,  51],\n",
       "          ...,\n",
       "          [ 17,  17,  16,  ...,  37,  35,  35],\n",
       "          [ 17,  17,  15,  ...,  36,  33,  32],\n",
       "          [ 17,  17,  15,  ...,  33,  31,  30]],\n",
       "\n",
       "         [[241, 241, 241,  ...,  84,  84,  83],\n",
       "          [241, 241, 241,  ...,  85,  85,  84],\n",
       "          [240, 240, 240,  ...,  86,  85,  85],\n",
       "          ...,\n",
       "          [ 25,  25,  26,  ...,  48,  49,  50],\n",
       "          [ 25,  25,  26,  ...,  45,  46,  46],\n",
       "          [ 25,  25,  26,  ...,  42,  43,  44]],\n",
       "\n",
       "         [[ 99,  99,  99,  ...,   0,   0,   0],\n",
       "          [ 91,  91,  91,  ...,   1,   0,   0],\n",
       "          [ 81,  80,  80,  ...,   2,   1,   1],\n",
       "          ...,\n",
       "          [ 12,  12,  12,  ...,  29,  27,  26],\n",
       "          [ 12,  12,  12,  ...,  27,  25,  23],\n",
       "          [ 12,  12,  12,  ...,  23,  22,  21]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "daisy\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(test_image.float())\n",
    "    output = output.argmax(axis=1)\n",
    "    print(output)\n",
    "\n",
    "#converting the output to flower name\n",
    "if output == 0:\n",
    "    flower = 'daisy'\n",
    "    \n",
    "print(flower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image1=convert_image_to_tensor('sunflower.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n",
      "sunflower\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(test_image1.float())\n",
    "    output = output.argmax(axis=1)\n",
    "    print(output)\n",
    "    if output == 3:\n",
    "        flower = 'sunflower'\n",
    "\n",
    "print(flower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
